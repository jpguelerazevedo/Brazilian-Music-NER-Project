# An√°lise Comparativa de NER em Letras de M√∫sicas: spaCy vs. BERT

Este projeto de Processamento de Linguagem Natural (PLN) treina e compara dois modelos customizados para a tarefa de Reconhecimento de Entidades Nomeadas (NER). O objetivo √© extrair e classificar entidades como Pessoas (`PER`), Lugares (`LOC`), A√ß√µes (`ACT`), Tempo (`TEMP`), Produtos (`PRODUTO`) e M√≠dia (`MIDIA`) das letras da banda brasileira Engenheiros do Hawaii.

1.  Um modelo treinado do zero com a biblioteca **spaCy**.
2.  Um modelo pr√©-treinado **BERT (BERTimbau)** que passa por um processo de fine-tuning com a biblioteca **Hugging Face Transformers**.

## Tecnologias Utilizadas
* Python 3.9+
* spaCy 3.x
* Hugging Face Transformers
* PyTorch
* Datasets & Evaluate (Hugging Face)

## Como Rodar o Projeto: Passo a Passo

Siga estas instru√ß√µes para configurar o ambiente e depois treinar e avaliar cada modelo.

### 1. Configura√ß√£o do Ambiente (Setup Comum)

Estes passos s√£o necess√°rios para ambos os modelos.

**1.1. Clone o Reposit√≥rio:**
```bash
git clone [https://github.com/seu-usuario/seu-repositorio.git](https://github.com/seu-usuario/seu-repositorio.git)
```

**1.2. Navegue at√© a Pasta de Trabalho:**
√â crucial executar todos os comandos de dentro da subpasta `spacyConfig`.
```bash
cd seu-repositorio/NER/spacyConfig
```

**1.3. Instale as Depend√™ncias:**
```bash
pip install -r requirements.txt
```

### 2. Op√ß√£o A: Treinando e Usando o Modelo spaCy

**2.1. Download do Modelo Base:**
```bash
python -m spacy download pt_core_news_lg
```

**2.2. Prepara√ß√£o dos Dados:**
```bash
python preparar_dados.py
```

**2.3. Treinamento:**
```bash
python -m spacy train config.cfg --output ./output --paths.train ./train.spacy --paths.dev ./dev.spacy
```

**2.4. Execu√ß√£o da An√°lise:**
```bash
python mainSpacy.py
```
*Um relat√≥rio chamado `relatorio_final_Spacy.txt` ser√° gerado.*

### 3. Op√ß√£o B: Treinando e Usando o Modelo BERT

**3.1. Treinamento (Fine-tuning):**
```bash
python bertFinetunning.py
```
*O modelo final ser√° salvo na pasta `bert-ner-enghaw`.*

**3.2. Execu√ß√£o da An√°lise:**
```bash
python mainBert.py
```
*Um relat√≥rio chamado `relatorio_final_BERT.txt` ser√° gerado.*

---

## üèÅ Resultados Finais e Compara√ß√£o

O objetivo final √© comparar as m√©tricas de performance de cada modelo para determinar qual abordagem se saiu melhor nesta tarefa espec√≠fica.

### Resultados - Modelo spaCy

O treinamento com spaCy (otimizado para `accuracy`) foi um **sucesso**.

* **Pontua√ß√£o M√°xima (F1-Score):** O modelo atingiu um `SCORE` de **0.83**.
* **Aprendizado:** O `LOSS NER` chegou a `0.00`, confirmando que o modelo aprendeu perfeitamente os exemplos.
* **Perfil:** No seu pico, o modelo demonstrou alt√≠ssima **Precis√£o (92.31%)** e bom **Recall (75.00%)**, indicando que √© um modelo muito confi√°vel.

**Tabela de Progresso (spaCy):**
```
 E     #     LOSS TOK2VEC  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE
---  ------  ------------  --------  ------  ------  ------  ------
  0       0          0.00     65.81    0.00    0.00    0.00    0.00
 32     200         13.79   1272.75   82.76   92.31   75.00    0.83
 72     400         22.62     67.94   77.42   80.00   75.00    0.77
121     600         14.23     32.41   75.00   75.00   75.00    0.75
180     800          3.14      5.84   66.67   81.82   56.25    0.67
247    1000         29.81     44.99   66.67   81.82   56.25    0.67
338    1200         37.74     53.58   78.57   91.67   68.75    0.79
438    1400          6.15      9.35   75.86   84.62   68.75    0.76
538    1600          0.00      0.00   75.86   84.62   68.75    0.76
712    1800          0.00      0.00   75.86   84.62   68.75    0.76
```

### Resultados - Modelo BERT

O fine-tuning do modelo BERT (BERTimbau) tamb√©m foi um **grande sucesso**, atingindo uma performance geral superior √† do modelo spaCy.

* **Pontua√ß√£o Final (F1-Score):** **0.87** (`eval_f1: 0.8667`)
* **Precis√£o (`eval_precision`):** **92.86%**
* **Recall (`eval_recall`):** **81.25%**

**An√°lise Resumida:**
Com um F1-Score de **0.87**, o modelo BERT se mostrou o vencedor nesta compara√ß√£o. Ele n√£o s√≥ atingiu uma **Precis√£o alt√≠ssima (92.86%)**, similar √† do spaCy, mas tamb√©m obteve um **Recall significativamente maior (81.25% vs 75.00% do spaCy)**. Isso significa que, al√©m de ser muito confi√°vel quando identifica uma entidade, o modelo BERT foi menos "t√≠mido" e conseguiu **encontrar mais entidades** corretas no texto, tornando-o o modelo mais completo e eficaz para esta tarefa.

**Por√©m, √© importante notar um desafio t√©cnico:** o modelo BERT, por sua natureza, quebra palavras desconhecidas em "subpalavras" (tokens), que aparecem com um prefixo `##` (ex: `Fidel` -> `Fi` + `##del`), o que inicialmente "danificou" a legibilidade do relat√≥rio de entidades. Isso demonstra que, embora o modelo BERT seja mais poderoso, ele pode ter complica√ß√µes em sua execu√ß√£o em compara√ß√£o com a sa√≠da mais direta do spaCy.

Para uma an√°lise visual e interativa dos gr√°ficos de treinamento do BERT, utilize o TensorBoard:
```bash
# Execute em um novo terminal, na pasta spacyConfig
tensorboard --logdir ./bert-ner-enghaw
```
*Abra o link `http://localhost:6006/` no seu navegador para ver o dashboard.*

---
_Este README documenta o fluxo de trabalho de um projeto de PLN para an√°lise e compara√ß√£o de modelos NER em letras musicais._
